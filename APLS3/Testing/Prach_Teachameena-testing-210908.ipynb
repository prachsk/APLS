{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE`:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "NAME = \"Prach Techameena\"\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing\n",
    "\n",
    "Testing is one of the most important components of sustainable software development. It improves code quality, maintainability and lifetime, and saves developer time. In previous labs you have already come across this in the form of assert statements. There are a number of testing libraries to support Python development and they all have in common that they build on the assert statement. In addition they provide better information when errors are found and the facilitate automated testing of large projects.\n",
    "\n",
    "We will use the doctest and the pytest libraries. If pytest is not installed you need to install it with pip. On your computers do this in the virtual environment where you previously installed jupyter\n",
    "\n",
    "~~~\n",
    "$ source venv/bin/activate   # windows: venv\\scripts\\activate.exe\n",
    "(venv)$ pip install pytest\n",
    "(venv)$ jupyter notebook\n",
    "~~~\n",
    "\n",
    "Testing libraries are typically designed to be used on Python source files while they are not adapted to be used with Jupyter notebooks. To be able to work with this in this lab, we use cell magic commands (%%file) to save cells in ordinary files and then execute pytest on those files\n"
   ],
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1504ad3e461601f18b0d96897be41a7",
     "grade": false,
     "grade_id": "cell-c374456facd20e46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import doctest\n",
    "import pytest\n",
    "# This is for jupyter to recognize changes in external files without restarting kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b419f62e162c6a542eeacf8bdb910bba",
     "grade": false,
     "grade_id": "cell-33440c27e09b63e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Assignment 1: add time stamps"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "%%file timestamps.py\n",
    "def sum_timestamps(l):\n",
    "    \"\"\"\n",
    "    >>> sum_timestamps(['5:32', '4:48'])\n",
    "    '10:20'\n",
    "    >>> sum_timestamps(['03:10', '01:00'])\n",
    "    '4:10'\n",
    "    >>> sum_timestamps(['2:10', '1:59'])\n",
    "    '4:09'\n",
    "    >>> sum_timestamps(['15:32', '45:48'])\n",
    "    '1:01:20'\n",
    "    >>> sum_timestamps(['6:15:32', '2:45:48'])\n",
    "    '9:01:20'\n",
    "    >>> sum_timestamps(['6:35:32', '2:45:48', '40:10'])\n",
    "    '10:01:30'\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    ################\n",
    "    sum_t = 0\n",
    "    for time in l:\n",
    "        time_unit = time.split(':')\n",
    "        if len(time.split(':')) == 2:\n",
    "            if int(time_unit[0]) > 12:\n",
    "                sum_t += int(time_unit[0])*60 + int(time_unit[1])\n",
    "            else:\n",
    "                sum_t += int(time_unit[0])*3600 + int(time_unit[1])*60\n",
    "        else:\n",
    "            sum_t += int(time_unit[0])*3600 + int(time_unit[1])*60 + int(time_unit[2])\n",
    "    \n",
    "    if sum_t%60 == 0:\n",
    "       return \"%d:%02d\" % (sum_t / 3600, sum_t / 60 % 60) \n",
    "    else:\n",
    "        return \"%d:%02d:%02d\" % (sum_t / 3600, sum_t / 60 % 60, sum_t % 60)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting timestamps.py\n"
     ]
    }
   ],
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b091c7219eb44034283ac9e92858b70",
     "grade": false,
     "grade_id": "cell-0ea01ce4cc34e8ff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import timestamps\n",
    "doctest.testmod(timestamps, verbose=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trying:\n",
      "    sum_timestamps(['5:32', '4:48'])\n",
      "Expecting:\n",
      "    '10:20'\n",
      "ok\n",
      "Trying:\n",
      "    sum_timestamps(['03:10', '01:00'])\n",
      "Expecting:\n",
      "    '4:10'\n",
      "ok\n",
      "Trying:\n",
      "    sum_timestamps(['2:10', '1:59'])\n",
      "Expecting:\n",
      "    '4:09'\n",
      "ok\n",
      "Trying:\n",
      "    sum_timestamps(['15:32', '45:48'])\n",
      "Expecting:\n",
      "    '1:01:20'\n",
      "ok\n",
      "Trying:\n",
      "    sum_timestamps(['6:15:32', '2:45:48'])\n",
      "Expecting:\n",
      "    '9:01:20'\n",
      "ok\n",
      "Trying:\n",
      "    sum_timestamps(['6:35:32', '2:45:48', '40:10'])\n",
      "Expecting:\n",
      "    '10:01:30'\n",
      "ok\n",
      "1 items had no tests:\n",
      "    timestamps\n",
      "1 items passed all tests:\n",
      "   6 tests in timestamps.sum_timestamps\n",
      "6 tests in 2 items.\n",
      "6 passed and 0 failed.\n",
      "Test passed.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=6)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a869f4cd39aaf8cb6b1d3e9635ce59a9",
     "grade": true,
     "grade_id": "cell-210600fd395cba29",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Assignment 2: time_stamps with pytest\n",
    "\n",
    "Write a test file to be used with pytest for assignment 1, with the same test cases."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "%%file test_timestamps.py\n",
    "import timestamps\n",
    "from timestamps import sum_timestamps\n",
    "# YOUR CODE HERE\n",
    "################\n",
    "def test_seum_timestamps():\n",
    "    assert sum_timestamps(['5:32', '4:48']) == '10:20'\n",
    "    assert sum_timestamps(['03:10', '01:00']) == '4:10'\n",
    "    assert sum_timestamps(['2:10', '1:59']) == '4:09'\n",
    "    assert sum_timestamps(['15:32', '45:48']) == '1:01:20'\n",
    "    assert sum_timestamps(['6:15:32', '2:45:48']) == '9:01:20'\n",
    "    assert sum_timestamps(['6:35:32', '2:45:48', '40:10']) == '10:01:30'\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting test_timestamps.py\n"
     ]
    }
   ],
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f15f003d3deb585ca2cbcb1bba918eea",
     "grade": false,
     "grade_id": "cell-ed7cd15078053e50",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "pytest.main(['-v', 'test_timestamps.py'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================= test session starts ==============================\n",
      "platform darwin -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 -- /usr/local/anaconda3/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/pax/Documents/School/MTLS/KTH/APLS3/Testing\n",
      "plugins: anyio-2.2.0\n",
      "collecting ... collected 1 item\n",
      "\n",
      "test_timestamps.py::test_seum_timestamps PASSED                          [100%]\n",
      "\n",
      "============================== 1 passed in 0.01s ===============================\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee29c06d79b8bcedd807baa0cfac3473",
     "grade": true,
     "grade_id": "cell-c597b267d405be1f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Assignment 3: running mean\n",
    "\n",
    "This is an example use of parametrized test cases. Look at the test function and understand how it works.\n",
    "Write a function that passes the tests\n",
    "\n"
   ],
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aff4a31a576f7a13a7f174e005cf620b",
     "grade": false,
     "grade_id": "cell-a4c4bdbc6f98dac1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "%%file test_running_mean.py\n",
    "import pytest\n",
    "\n",
    "from running_mean import running_mean\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"input_argument, expected_return\", [\n",
    "    ([1, 2, 3], [1, 1.5, 2]),\n",
    "    ([2, 6, 10, 8, 11, 10],\n",
    "     [2.0, 4.0, 6.0, 6.5, 7.4, 7.83]),\n",
    "    ([3, 4, 6, 2, 1, 9, 0, 7, 5, 8],\n",
    "     [3.0, 3.5, 4.33, 3.75, 3.2, 4.17, 3.57, 4.0, 4.11, 4.5]),\n",
    "    ([], []),\n",
    "])\n",
    "def test_running_mean(input_argument, expected_return):\n",
    "    ret = list(running_mean(input_argument))\n",
    "    assert ret == expected_return"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting test_running_mean.py\n"
     ]
    }
   ],
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f15c3812d0a4c64c65dfe19947283f0b",
     "grade": false,
     "grade_id": "cell-a62f5af5275891d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "%%file running_mean.py\n",
    "def running_mean(sequence):\n",
    "    \"\"\"Calculate the running mean of the sequence passed in,\n",
    "       returns a sequence of same length with the averages.\n",
    "       You can assume all items in sequence are numeric.\"\"\"\n",
    "  # YOUR CODE HERE\n",
    "  ################\n",
    "    total = 0\n",
    "    result = []\n",
    "    i = 1\n",
    "    for num in sequence:\n",
    "      total += num\n",
    "      ave = total/i\n",
    "      result.append(round(ave,2))\n",
    "      i+=1\n",
    "    return result\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting running_mean.py\n"
     ]
    }
   ],
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "772c3fe89217aea5bc44c3753181dbbe",
     "grade": false,
     "grade_id": "cell-96c10ee58ecac500",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "pytest.main(['-v', 'test_running_mean.py'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================= test session starts ==============================\n",
      "platform darwin -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 -- /usr/local/anaconda3/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/pax/Documents/School/MTLS/KTH/APLS3/Testing\n",
      "plugins: anyio-2.2.0\n",
      "collecting ... collected 4 items\n",
      "\n",
      "test_running_mean.py::test_running_mean[input_argument0-expected_return0] PASSED [ 25%]\n",
      "test_running_mean.py::test_running_mean[input_argument1-expected_return1] PASSED [ 50%]\n",
      "test_running_mean.py::test_running_mean[input_argument2-expected_return2] PASSED [ 75%]\n",
      "test_running_mean.py::test_running_mean[input_argument3-expected_return3] PASSED [100%]\n",
      "\n",
      "============================== 4 passed in 0.02s ===============================\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2214c633a7a294cb33de7bbac7d2bad5",
     "grade": true,
     "grade_id": "cell-6a057ac659530fd9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Assignment 4\n",
    "\n",
    "Write a second version of the test function for timestamp where the different test cases are different parameterizations for one test function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "%%file test_timestamps_2.py\n",
    "import pytest\n",
    "from timestamps import sum_timestamps\n",
    "\n",
    "# YOUR CODE HERE\n",
    "################\n",
    "@pytest.mark.parametrize(\"input_argument, expected_return\", [\n",
    "    (['5:32', '4:48'], '10:20'),\n",
    "    (['03:10', '01:00'],'4:10'),\n",
    "    (['2:10', '1:59'], '4:09'),\n",
    "    (['15:32', '45:48'], '1:01:20'),\n",
    "    (['6:15:32', '2:45:48'], '9:01:20'),\n",
    "    (['6:35:32', '2:45:48', '40:10'], '10:01:30'),\n",
    "])\n",
    "def test_timestamps_2(input_argument, expected_return):\n",
    "    assert sum_timestamps(input_argument) == expected_return"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting test_timestamps_2.py\n"
     ]
    }
   ],
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad40885363cb8747ef38c820b6420f30",
     "grade": false,
     "grade_id": "cell-1d068aaf56835af8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "pytest.main(['-v', 'test_timestamps_2.py'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================= test session starts ==============================\n",
      "platform darwin -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 -- /usr/local/anaconda3/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/pax/Documents/School/MTLS/KTH/APLS3/Testing\n",
      "plugins: anyio-2.2.0\n",
      "collecting ... collected 6 items\n",
      "\n",
      "test_timestamps_2.py::test_timestamps_2[input_argument0-10:20] PASSED    [ 16%]\n",
      "test_timestamps_2.py::test_timestamps_2[input_argument1-4:10] PASSED     [ 33%]\n",
      "test_timestamps_2.py::test_timestamps_2[input_argument2-4:09] PASSED     [ 50%]\n",
      "test_timestamps_2.py::test_timestamps_2[input_argument3-1:01:20] PASSED  [ 66%]\n",
      "test_timestamps_2.py::test_timestamps_2[input_argument4-9:01:20] PASSED  [ 83%]\n",
      "test_timestamps_2.py::test_timestamps_2[input_argument5-10:01:30] PASSED [100%]\n",
      "\n",
      "============================== 6 passed in 0.03s ===============================\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77cf3d292ff9c2ea8171ce5f579ccad6",
     "grade": true,
     "grade_id": "cell-32b906546d801ca5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "e134e05457d34029b6460cd73bbf1ed73f339b5b6d98c95be70b69eba114fe95"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}